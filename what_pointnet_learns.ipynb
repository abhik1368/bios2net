{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import plotly.express as px\n",
    "import plotly.graph_objects as go\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ! cd tf_ops; bash compile_ops.sh"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ! git reset --hard\n",
    "# ! git pull"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "! git diff"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!git log"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "def plot_conf_matrix(confusion_matrix, normalize=True):\n",
    "    plt.figure(figsize=(20, 19.5))  # width by height\n",
    "    if normalize:\n",
    "        col_norm = np.sum(confusion_matrix, axis=0)\n",
    "        confusion_matrix /= col_norm[None, :]\n",
    "    ax = sns.heatmap(confusion_matrix, #annot=True, annot_kws={'size': 3},\n",
    "                    fmt='.1f', cbar=False, cmap='binary', linecolor='black', linewidths=0.5)\n",
    "    return plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow.compat.v1 as tf\n",
    "tf.disable_v2_behavior()\n",
    "tf.enable_eager_execution()\n",
    "tf.executing_eagerly()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import argparse\n",
    "import math\n",
    "from datetime import datetime, timezone\n",
    "import h5py\n",
    "import numpy as np\n",
    "\n",
    "import socket\n",
    "import importlib\n",
    "import os\n",
    "import sys\n",
    "BASE_DIR = '/scidatalg/ar/pointnet2'\n",
    "ROOT_DIR = BASE_DIR\n",
    "sys.path.append(BASE_DIR)\n",
    "sys.path.append(os.path.join(ROOT_DIR, 'models'))\n",
    "sys.path.append(os.path.join(ROOT_DIR, 'utils'))\n",
    "from utils import provider\n",
    "import tf_util\n",
    "import pfr_dataset\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "EPOCH_CNT = 0\n",
    "\n",
    "MODEL = 'models.pointnet2_super'\n",
    "DATASET_PATH = 'data/eDD_scaled_splited53/'\n",
    "BATCH_SIZE = 32\n",
    "NUM_POINT = 1024\n",
    "MAX_EPOCH = 10\n",
    "BASE_LEARNING_RATE = 0.001\n",
    "GPU_INDEX = 1\n",
    "MOMENTUM = 0.9\n",
    "OPTIMIZER = 'adam'\n",
    "DECAY_STEP = 200000\n",
    "DECAY_RATE = 0.9\n",
    "WEIGHT_DECAY = 0.001\n",
    "NUM_CLASSES=27\n",
    "NO_SHUFFLE_POINTS = True\n",
    "WANDB = False\n",
    "ADD_N_C = True\n",
    "TO_CATEGORICAL_IND = [8]\n",
    "TO_CATEGORICAL_SIZES = [20]\n",
    "LOG_DIR = 'logs'\n",
    "\n",
    "\n",
    "MODEL = importlib.import_module(MODEL) # import network module\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# MODEL_FILE = os.path.join(ROOT_DIR, 'models', MODEL +'.py')\n",
    "if not os.path.exists(LOG_DIR): os.mkdir(LOG_DIR)\n",
    "LOG_FOUT = open(os.path.join(LOG_DIR, 'log_train.txt'), 'w')\n",
    "\n",
    "BN_INIT_DECAY = 0.5\n",
    "BN_DECAY_DECAY_RATE = 0.5\n",
    "BN_DECAY_DECAY_STEP = float(DECAY_STEP)\n",
    "BN_DECAY_CLIP = 0.99\n",
    "\n",
    "HOSTNAME = socket.gethostname()\n",
    "\n",
    "TRAIN_DATASET = pfr_dataset.PFRDataset(\n",
    "    root=DATASET_PATH,\n",
    "    batch_size=BATCH_SIZE,\n",
    "    npoints=NUM_POINT,\n",
    "    split='train',\n",
    "    normalize=False,\n",
    "    normal_channel=True,\n",
    "    shuffle_points=not NO_SHUFFLE_POINTS,\n",
    "    add_n_c_info=ADD_N_C,\n",
    "    to_categorical_indexes=TO_CATEGORICAL_IND,\n",
    "    to_categorical_sizes=TO_CATEGORICAL_SIZES\n",
    ")\n",
    "TEST_DATASET = pfr_dataset.PFRDataset(\n",
    "    root=DATASET_PATH,\n",
    "    batch_size=BATCH_SIZE,\n",
    "    npoints=NUM_POINT,\n",
    "    split='test',\n",
    "    normalize=False,\n",
    "    normal_channel=True,\n",
    "    add_n_c_info=ADD_N_C,\n",
    "    to_categorical_indexes=TO_CATEGORICAL_IND,\n",
    "    to_categorical_sizes=TO_CATEGORICAL_SIZES\n",
    ")\n",
    "\n",
    "FEATURES_CHANNELS = TRAIN_DATASET.num_channel() - 3\n",
    "LABELS = [i[0] for i in sorted(TRAIN_DATASET.classes.items(), key=lambda item: item[1])]\n",
    "\n",
    "print(f'Database created with {FEATURES_CHANNELS} features channels')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = TRAIN_DATASET.next_batch(augment=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a[0].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def get_timestamp():\n",
    "    timestamp = str(datetime.now(timezone.utc))[:16]\n",
    "    timestamp = timestamp.replace('-', '')\n",
    "    timestamp = timestamp.replace(' ', '_')\n",
    "    timestamp = timestamp.replace(':', '')\n",
    "    return timestamp\n",
    "\n",
    "INIT_TIMESTAMP = get_timestamp()\n",
    "\n",
    "if WANDB:\n",
    "    import wandb\n",
    "    wandb.init(project='pointnet_pfr', name=INIT_TIMESTAMP)\n",
    "\n",
    "def log_string(out_str):\n",
    "    LOG_FOUT.write(out_str+'\\n')\n",
    "    LOG_FOUT.flush()\n",
    "    print(out_str)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Eval trained model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow.compat.v1 as tf\n",
    "\n",
    "tf.disable_v2_behavior()\n",
    "import numpy as np\n",
    "import argparse\n",
    "import socket\n",
    "import importlib\n",
    "import time\n",
    "import os\n",
    "import scipy.misc\n",
    "import sys\n",
    "ROOT_DIR = BASE_DIR\n",
    "sys.path.append(BASE_DIR)\n",
    "sys.path.append(os.path.join(ROOT_DIR, 'models'))\n",
    "sys.path.append(os.path.join(ROOT_DIR, 'utils'))\n",
    "import provider"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "tf.enable_eager_execution()\n",
    "tf.executing_eagerly()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "! ls EDD_aux3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "MODEL_PATH = 'EDD_aux3/26/model.ckpt'\n",
    "NUM_CLASSES = 27\n",
    "\n",
    "tf.reset_default_graph()\n",
    "MODEL = importlib.import_module('EDD_aux3.pointnet2_super_backup')\n",
    "is_training = False\n",
    "with tf.Graph().as_default():\n",
    "\n",
    "\n",
    "    with tf.device('/gpu:'+str(GPU_INDEX)):\n",
    "        pointclouds_pl, labels_pl = MODEL.placeholder_inputs(BATCH_SIZE, NUM_POINT, 3 + FEATURES_CHANNELS)\n",
    "        is_training_pl = tf.placeholder(tf.bool, shape=())\n",
    "        \n",
    "        batch = tf.get_variable('batch', [],\n",
    "        initializer=tf.constant_initializer(0), trainable=False)\n",
    "#         bn_decay = get_bn_decay(batch)\n",
    "#         tf.summary.scalar('bn_decay', bn_decay)\n",
    "\n",
    "        # simple model\n",
    "        pt_pred, pred, end_points = MODEL.get_model(pointclouds_pl, is_training_pl, NUM_CLASSES, weight_decay=WEIGHT_DECAY, knn=False)        \n",
    "        MODEL.get_loss(pt_pred, pred, labels_pl, end_points)\n",
    "        losses = tf.get_collection('losses')\n",
    "        total_loss = tf.add_n(losses, name='total_loss')\n",
    "        tf.summary.scalar('total_loss', total_loss)\n",
    "        for l in losses + [total_loss]:\n",
    "            tf.summary.scalar(l.op.name, l)\n",
    "\n",
    "        # Add ops to save and restore all the variables.\n",
    "        saver = tf.train.Saver()\n",
    "\n",
    "    # Create a session\n",
    "    config = tf.ConfigProto()\n",
    "    config.gpu_options.allow_growth = True\n",
    "    config.allow_soft_placement = True\n",
    "    config.log_device_placement = False\n",
    "    sess = tf.Session(config=config)\n",
    "\n",
    "    # Restore variables from disk.\n",
    "    saver.restore(sess, MODEL_PATH)\n",
    "\n",
    "    ops = {'pointclouds_pl': pointclouds_pl,\n",
    "           'labels_pl': labels_pl,\n",
    "           'is_training_pl': is_training_pl,\n",
    "           'pred': pred,\n",
    "           'pt_pred': pt_pred,\n",
    "           'end_points': end_points,\n",
    "           'loss': total_loss}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.enable_eager_execution()\n",
    "tf.executing_eagerly()\n",
    "end_points"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "TEST_DATASET = pfr_dataset.PFRDataset(\n",
    "    root='data/eDD_scaled_splited53/',\n",
    "    batch_size=32,\n",
    "    npoints=1024,\n",
    "    split='test',\n",
    "    normalize=False,\n",
    "    normal_channel=True,\n",
    "    shuffle_points=False,\n",
    "    shuffle=False,\n",
    "    add_n_c_info=True,\n",
    "    to_categorical_indexes=TO_CATEGORICAL_IND,\n",
    "    to_categorical_sizes=TO_CATEGORICAL_SIZES,\n",
    "#     omit_parameters_ranges=[3, 9]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# go.Figure(go.Heatmap(z=TEST_DATASET[0][0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_vectors = []\n",
    "\n",
    "is_training = False\n",
    "\n",
    "NUM_CLASSES = 27\n",
    "\n",
    "# Make sure batch data is of same size\n",
    "cur_batch_data = np.zeros((BATCH_SIZE,NUM_POINT,TRAIN_DATASET.num_channel()))\n",
    "cur_batch_label = np.zeros((BATCH_SIZE), dtype=np.int32)\n",
    "\n",
    "confusion_matrix = np.zeros((NUM_CLASSES, NUM_CLASSES))\n",
    "pt_confusion_matrix = np.zeros((NUM_CLASSES, NUM_CLASSES))\n",
    "\n",
    "loss_sum = 0\n",
    "top3_correct = 0\n",
    "top3_class_correct = np.zeros((NUM_CLASSES))\n",
    "pt_top3_correct = 0\n",
    "pt_top3_class_correct = np.zeros((NUM_CLASSES))\n",
    "\n",
    "batch_idx = 0\n",
    "\n",
    "log_string(str(datetime.now()))\n",
    "log_string(f'---- EPOCH {EPOCH_CNT:03d} EVALUATION ----')\n",
    "\n",
    "pred_vectors = []\n",
    "pointnet_input = []\n",
    "failed = []\n",
    "\n",
    "while TEST_DATASET.has_next_batch():\n",
    "    batch_data, batch_label = TEST_DATASET.next_batch(augment=False)\n",
    "    bsize = batch_data.shape[0]\n",
    "    # for the last batch in the epoch, the bsize:end are from last batch\n",
    "    cur_batch_data[0:bsize,...] = batch_data\n",
    "    cur_batch_label[0:bsize] = batch_label\n",
    "\n",
    "    feed_dict = {ops['pointclouds_pl']: cur_batch_data,\n",
    "                 ops['labels_pl']: cur_batch_label,\n",
    "                 ops['is_training_pl']: is_training}\n",
    "    loss_val, pred_val, pt_pred, end_points = sess.run([ops['loss'], ops['pred'], ops['pt_pred'], ops['end_points']], feed_dict=feed_dict)\n",
    "    pred_vectors.extend(pred_val[:bsize])\n",
    "#     pointnet_input.extend(end_points['points'][:bsize])\n",
    "    feature_vectors.extend(end_points['feature_vector'][:bsize])\n",
    "    pred_val_arg = np.argmax(pred_val, 1)\n",
    "    loss_sum += loss_val\n",
    "    batch_idx += 1\n",
    "    for i in range(0, bsize):\n",
    "        l = batch_label[i]\n",
    "        top3 = pred_val[i].argsort()[-3:][::-1]\n",
    "        failed.append(1 if l==top3[0] else 1 if l==top3[1] else 1 if l==top3[2] else 5)\n",
    "        top3_correct += l in top3\n",
    "        top3_class_correct[l] += l in top3\n",
    "        confusion_matrix[pred_val_arg[i], l] += 1\n",
    "    \n",
    "    # for pointnet \n",
    "#     pred_val_arg = np.argmax(pt_pred, 1)\n",
    "#     loss_sum += loss_val\n",
    "#     batch_idx += 1\n",
    "#     for i in range(0, bsize):\n",
    "#         l = batch_label[i]\n",
    "#         pt_top3 = pt_pred[i].argsort()[-3:][::-1]\n",
    "# #         pt_failed.append(1 if l==top3[0] else 1 if l==top3[1] else 1 if l==top3[2] else 5)\n",
    "#         pt_top3_correct += l in pt_top3\n",
    "#         pt_top3_class_correct[l] += l in pt_top3\n",
    "#         pt_confusion_matrix[pred_val_arg[i], l] += 1\n",
    "        \n",
    "\n",
    "col_norm = np.maximum(np.nan_to_num(np.sum(confusion_matrix, axis=0)), 1)\n",
    "accuracy = np.sum(np.diag(confusion_matrix)) / np.sum(confusion_matrix)\n",
    "top3_accuracy = top3_correct / np.sum(confusion_matrix)\n",
    "top3_avg_class_acc = np.mean(top3_class_correct / col_norm)\n",
    "class_acc = np.diag(confusion_matrix) / col_norm\n",
    "avg_class_acc = np.mean(np.nan_to_num(class_acc))\n",
    "\n",
    "# for pointnet\n",
    "col_norm = np.maximum(np.nan_to_num(np.sum(pt_confusion_matrix, axis=0)), 1)\n",
    "pt_accuracy = np.sum(np.diag(pt_confusion_matrix)) / np.sum(pt_confusion_matrix)\n",
    "pt_top3_accuracy = pt_top3_correct / np.sum(pt_confusion_matrix)\n",
    "pt_class_acc = np.diag(confusion_matrix) / col_norm\n",
    "\n",
    "\n",
    "log_string(f'eval mean loss: {loss_sum / 50}')\n",
    "log_string(f'eval accuracy: {accuracy}')\n",
    "log_string(f'eval avg class acc {avg_class_acc}')\n",
    "log_string(f'eval top3 acc {top3_accuracy}')\n",
    "log_string(f'eval top3 avg class acc {top3_avg_class_acc}')\n",
    "\n",
    "log_string(f'\\neval accuracy: {pt_accuracy}')\n",
    "log_string(f'eval top3 acc {pt_top3_accuracy}')\n",
    "\n",
    "\n",
    "EPOCH_CNT += 1\n",
    "\n",
    "TEST_DATASET.reset()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "go.Figure(go.Heatmap(z=(np.array(feature_vectors) > 0).astype(int)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.manifold import TSNE\n",
    "\n",
    "tsne = TSNE(n_components=2, perplexity=20)\n",
    "X = tsne.fit_transform(feature_vectors)\n",
    "\n",
    "labels = [i[0] for i in TEST_DATASET.datapath]\n",
    "pdb = [i[1].split('/')[4][1:5] for i in TEST_DATASET.datapath]\n",
    "classes = [i[0] for i in labels]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dict(\n",
    "    labels = labels,\n",
    "    pdb = pdb,\n",
    "    classes = classes,\n",
    "    x = X[:, 0],\n",
    "    y = X[:, 1],\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "px.scatter(x=X[:, 0], y=X[:, 1], color=labels, size=failed, hover_data=[pdb])\\\n",
    ".update_layout(width=900, height=900)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "end_points.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "end_points['points'].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "temp_ker = end_points['temp1_ker']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pt_ker = end_points['pt_ker']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "inp = np.concatenate([end_points['l0_xyz'], end_points['points']], axis=-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "inp.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = end_points['input_point_cloud'][0, :, :]\n",
    "a.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "px.scatter_3d(x=a[:, 0], y=a[:, 1], z=a[:, 2], color=np.arange(1024))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## First kernel layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "end_points.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from glob import glob \n",
    "import plotly.express as px\n",
    "import plotly.graph_objects as go"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "SAVE_PATH = 'end_points.pkl'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "with open(SAVE_PATH, 'wb') as f:\n",
    "    pickle.dump(end_points, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ! cp end_points.pkl ~/F184_double_loss.pkl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch = TEST_DATASET.next_batch()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "AA_INDEX = {\n",
    "    'A': 0,\n",
    "    'C': 1,\n",
    "    'D': 2,\n",
    "    'E': 3,\n",
    "    'F': 4,\n",
    "    'G': 5,\n",
    "    'H': 6,\n",
    "    'I': 7,\n",
    "    'K': 8,\n",
    "    'L': 9,\n",
    "    'M': 10,\n",
    "    'N': 11,\n",
    "    'P': 12,\n",
    "    'Q': 13,\n",
    "    'R': 14,\n",
    "    'S': 15,\n",
    "    'T': 16,\n",
    "    'V': 17,\n",
    "    'W': 18,\n",
    "    'Y': 19\n",
    "}\n",
    "AA_INDEX = {v:k for k,v in AA_INDEX.items()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def produce_sizes(kernels, which_kernel, quantile=0.85):\n",
    "    q = np.quantile(kernels[:,which_kernel], q=quantile)\n",
    "    return [0.9 if i > q else 0.1 for i in kernels[:, which_kernel]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def expand_to_array(i, length):\n",
    "    arr = np.zeros(length)\n",
    "    arr[i] = 1\n",
    "    return arr \n",
    "real_vectors = np.array([expand_to_array(TEST_DATASET[i][1][0], 184) for i in range(len(TEST_DATASET))])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "end_points['pt_ker'].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "go.Figure(go.Heatmap(z=end_points['pt_ker'][0, 0]))\\\n",
    ".update_layout(dict(xaxis_title='Filters', yaxis_title='Features'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def draw_stacked_heatmap(heatmaps, y_titles, x_title):\n",
    "    fig = go.Figure()\n",
    "    cum_heights = np.cumsum([i.shape[0] for i in heatmaps])\n",
    "    final_heatmap = np.concatenate(heatmaps)\n",
    "    fig.add_traces([go.Heatmap(z=final_heatmap)])\n",
    "    \n",
    "    # add tick values on y axis\n",
    "    bins_ranges = [0] + list(np.cumsum([i.shape[0] for i in heatmaps]))\n",
    "    bins_middle = [(bins_ranges[i] + bins_ranges[i+1])/2 for i in range(len(bins_ranges)-1)]\n",
    "    fig.update_layout(\n",
    "        showlegend=False,\n",
    "        yaxis=dict(ticktext=y_titles, tickvals=bins_middle, title='Filter sizes'),\n",
    "        xaxis_title='Points'\n",
    "    )\n",
    "    \n",
    "    # add lines separating kernels\n",
    "    [heatmaps[i].shape[1] for i in range(len(heatmaps)-1)]\n",
    "    fig.add_traces([\n",
    "        go.Scatter(x=[-.5, heatmaps[i].shape[1] - 0.5], y=[cum_heights[i] - 0.5] * 2,\n",
    "                   mode='lines', line=dict(dash='dash', width=2, color='black'))\n",
    "        for i in range(len(heatmaps)-1)\n",
    "    ])\n",
    "    return fig"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "[i.shape for i in end_points['temp1_ker']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fives = [np.squeeze(end_points['temp1_ker'][2][:, :, :, i]) for i in range(16)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "[i.shape for i in end_points['temp1_ker']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "filters_titles = [r'$1 \\times 1$', r'$3 \\times 1$', r'$5 \\times 1$', r'$7 \\times 1$']\n",
    "for j in range(4):\n",
    "    kernels = np.squeeze(end_points['temp1_ker'][j], axis=1)\n",
    "    filters = [kernels[:, :, i] for i in range(kernels.shape[-1])]\n",
    "    draw_stacked_heatmap(filters, [filters_titles[j]] * kernels.shape[-1], 'a')\\\n",
    "    .show()\n",
    "    #     .add_traces([go.Scatter(x=[53, 53], y=[0, 111], mode='lines', line=dict(dash='dash', width=2, color='black'))])\\\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "end_points.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.expand_dims(end_points['temp1_input'][0, :, :], axis=(0,2)).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.expand_dims(end_points['temp1_input'][0, :, :], axis=(0,2)).shape\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def visualize_activations(point_cloud, kernels, kernel_titles):\n",
    "    activations = []\n",
    "    kernel_labels = []\n",
    "    for i, kernel in enumerate(kernels):\n",
    "        kernel_labels.extend([kernel_titles[i]] * kernel.shape[-1])\n",
    "        activation = tf.squeeze(\n",
    "            tf.nn.conv2d(\n",
    "                point_cloud, kernel, padding='SAME')\n",
    "        ).numpy()\n",
    "        activations.append(activation)\n",
    "\n",
    "    d = dict(kernel=[], x=[], y=[], z=[], aa=[], importance=[], ss=[], dist=[], activation=[], kernel_size=[])\n",
    "    point_cloud = np.squeeze(point_cloud)\n",
    "    for i, kernel_group in enumerate(kernels):\n",
    "        for kernel in range(kernel_group.shape[-1]):\n",
    "            d['activation'].extend(activations[i][:, kernel])\n",
    "            d['importance'].extend(produce_sizes(activations[i], kernel))\n",
    "            d['x'].extend(point_cloud[:, 0])\n",
    "            d['y'].extend(point_cloud[:, 1])\n",
    "            d['z'].extend(point_cloud[:, 2])\n",
    "            ss = np.zeros(point_cloud.shape[0])\n",
    "            ss += point_cloud[:, 5] * 17\n",
    "            ss += point_cloud[:, 6] * 21\n",
    "            d['ss'].extend(ss)\n",
    "            is_helix = point_cloud[:, 5]\n",
    "            is_beta = point_cloud[:, 6]\n",
    "            d['aa'].extend([AA_INDEX[i] for i in np.argmax(point_cloud[:, 8:28], axis=1)])\n",
    "            d['kernel'].extend([kernel] * point_cloud.shape[0])\n",
    "            d['kernel_size'].extend([kernel_titles[i]] * point_cloud.shape[0])\n",
    "            d['dist'].extend(np.linspace(0, 1, activations[i].shape[0]))\n",
    "        ss = np.zeros(shape=())\n",
    "    fig = draw_stacked_heatmap([i.T for i in activations], kernel_titles, 'points')\n",
    "    return activations, pd.DataFrame(d), fig\n",
    "\n",
    "activations, df, fig = visualize_activations(\n",
    "    np.expand_dims(end_points['temp1_input'][2, :, :], axis=(0,2)), \n",
    "    end_points['temp1_ker'], \n",
    "    [r'$1 \\times 1$', r'$3 \\times 1$', r'$5 \\times 1$', r'$7 \\times 1$']\n",
    ")\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = 3\n",
    "fr'ala{a}'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.loc[:, ['kernel', 'kernel_size']].apply(lambda x: )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.loc[:, 'kernel_id'] = df.kernel_size.str[:-1] + ' ' + df.kernel.astype(str) + '$'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = px.scatter_3d(df, x='x', y='y', z='z',\n",
    "                    animation_frame='kernel_id',\n",
    "                    symbol='ss', \n",
    "                    color='activation', \n",
    "                    size=df.activation - df.activation.min(),\n",
    "                   )\n",
    "\n",
    "axes_settings = dict(\n",
    "#     backgroundcolor=\"white\",\n",
    "    gridcolor=\"rgba(127,127,127,0.5)\",\n",
    "#     showbackground=False,\n",
    "    zerolinecolor=\"white\",\n",
    "    showticklabels=False,\n",
    "    title=''\n",
    ")\n",
    "\n",
    "fig.update_layout(\n",
    "    width=1000, height=1000,\n",
    "    scene = dict(\n",
    "        xaxis = axes_settings,\n",
    "        yaxis = axes_settings,\n",
    "        zaxis = axes_settings\n",
    "    ),\n",
    "    showlegend=False\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(1):\n",
    "    print(i)\n",
    "    kernel_heatmap, input_heatmap, df = visualize_conv(TEST_DATASET[i][0], [end_points['pt_ker']])\n",
    "    kernel_heatmap.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def visualize_pointnet(protein, kernels):\n",
    "    activations = []\n",
    "    kernel_labels = []\n",
    "    kernel_titles = [r'$1 \\times 1$', r'$3 \\times 1$', r'$5 \\times 1$', r'$7 \\times 1$']\n",
    "    for i, kernel in enumerate(kernels):\n",
    "        kernel_labels.extend([kernel_titles[i]] * kernel.shape[-1])\n",
    "        activation = tf.squeeze(\n",
    "            tf.nn.conv2d(\n",
    "                np.expand_dims(protein, (0,2)), kernel, padding='SAME')\n",
    "        ).numpy()\n",
    "        activations.append(activation)\n",
    "    activations = np.concatenate(activations, axis=1)\n",
    "    \n",
    "    kernels_heatmap = go.Figure(go.Heatmap(z=activations.T))\n",
    "    accumulated_out_channels = np.cumsum([i.shape[-1] for i in kernels])\n",
    "    kernels_heatmap.add_traces([\n",
    "        go.Scatter(x=[0, protein.shape[0]], y=[accumulated_out_channels[i] - 0.5] * 2,\n",
    "                   mode='lines', line=dict(dash='dash', width=3, color='black'))\n",
    "        for i in range(len(kernels)-1)\n",
    "    ])\n",
    "    bins_ranges = [0] + list(np.cumsum([i.shape[-1] for i in kernels]))\n",
    "    bins_middle = [(bins_ranges[i] + bins_ranges[i+1])/2 for i in range(len(bins_ranges)-1)]\n",
    "    kernels_heatmap.update_layout(\n",
    "        showlegend=False,\n",
    "        yaxis=dict(ticktext=kernel_titles, tickvals=bins_middle, title='Filter sizes'),\n",
    "        xaxis_title='Points'\n",
    "    )\n",
    "    \n",
    "    d = dict(kernel=[], x=[], y=[], z=[], aa=[], importance=[], ss=[], dist=[], activation=[])\n",
    "    for kernel in range(activations.shape[1]):\n",
    "        d['activation'].extend(activations[:, kernel])\n",
    "        d['importance'].extend(produce_sizes(activations, kernel))\n",
    "        d['x'].extend(protein[:, 0])\n",
    "        d['y'].extend(protein[:, 1])\n",
    "        d['z'].extend(protein[:, 2])\n",
    "        ss = np.zeros(protein.shape[0])\n",
    "        ss += protein[:, 32+5] * 17\n",
    "        ss += protein[:, 32+6] * 21\n",
    "        d['ss'].extend(ss)\n",
    "        is_helix = protein[:, 32+5]\n",
    "        is_beta = protein[:, 32+6]\n",
    "        d['aa'].extend([AA_INDEX[i] for i in protein[:, 32+8]])\n",
    "        d['kernel'].extend([kernel]*activations.shape[0])\n",
    "        d['dist'].extend(np.linspace(0, 1, activations.shape[0]))\n",
    "    ss = np.zeros(shape=())\n",
    "    input_heatmap = go.Figure(go.Heatmap(z=protein.T))\n",
    "    return kernels_heatmap, input_heatmap, pd.DataFrame(d)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(1):\n",
    "    print(i)\n",
    "    print(np.concatenate([TEST_DATASET[i][0][:, :3], pointnet_input[i]], axis=-1).shape)\n",
    "    kernel_heatmap, input_heatmap, df = visualize_pointnet(\n",
    "        np.concatenate([TEST_DATASET[i][0][:, :3], pointnet_input[i]], axis=-1), \n",
    "        [end_points['pt_ker']]\n",
    "    )\n",
    "    kernel_heatmap.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = px.scatter_3d(df, x='x', y='y', z='z',\n",
    "                    animation_frame='kernel',\n",
    "                    symbol='ss', \n",
    "                    color='activation', \n",
    "                    size=df.activation - df.activation.min(),\n",
    "                   )\n",
    "\n",
    "axes_settings = dict(\n",
    "#     backgroundcolor=\"white\",\n",
    "    gridcolor=\"rgba(127,127,127,0.5)\",\n",
    "#     showbackground=False,\n",
    "    zerolinecolor=\"white\",\n",
    "    showticklabels=False,\n",
    "    title=''\n",
    ")\n",
    "\n",
    "fig.update_layout(\n",
    "    width=1000, height=1000,\n",
    "    scene = dict(\n",
    "        xaxis = axes_settings,\n",
    "        yaxis = axes_settings,\n",
    "        zaxis = axes_settings\n",
    "    ),\n",
    "    showlegend=False\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Feature vectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.sum(feature_vectors[0] == 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import plotly.graph_objects as go\n",
    "import pandas as pd\n",
    "go.Figure(go.Heatmap(z=feature_vectors))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = TEST_DATASET.datapath\n",
    "\n",
    "sids = [i[1].split('/')[-1][:-4] for i in data]\n",
    "\n",
    "folds = [i[0] for i in data]\n",
    "\n",
    "df = pd.DataFrame({'fold': folds, 'sids':sids})\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_vectors[0].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.save('../EDD512_feature_vectors_for_tsne.npy', feature_vectors)\n",
    "df.to_csv('../EDD512_feature_vectors_for_tsne.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
